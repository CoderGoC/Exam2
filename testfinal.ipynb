{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import KFold, cross_val_score\n",
    "# from sklearn.linear_model import Ridge, Lasso, HuberRegressor, TheilSenRegressor\n",
    "# from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder, FunctionTransformer, KBinsDiscretizer\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "# # Load the datasets\n",
    "# train_df = pd.read_csv('train.csv')\n",
    "# test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# # Define the feature engineering function\n",
    "# def feature_funk(df):\n",
    "#     df['Surface Area'] = 2 * (df['Length'] * df['Diameter'] + df['Length'] * df['Height'] + df['Diameter'] * df['Height'])\n",
    "#     df['Volume'] = df['Length'] * df['Diameter'] * df['Height']\n",
    "#     df['Log Weight'] = df['Weight'].apply(lambda x: np.log(x + 1) if x > 0 else np.nan)\n",
    "#     df['Length Bins'] = pd.qcut(df['Length'], q=4, labels=False)\n",
    "\n",
    "#     sex = {'I': 1, 'F': 3, 'M': 2}\n",
    "#     df['Sex'] = df['Sex'].replace(sex)\n",
    "#     df = df[df['Sex'].isin([1, 2, 3])]\n",
    "#     df['Sex'] = pd.to_numeric(df['Sex'])\n",
    "#     return df\n",
    "\n",
    "# # Apply the feature engineering function to both datasets\n",
    "# train_df = feature_funk(train_df)\n",
    "# test_df = feature_funk(test_df)\n",
    "\n",
    "# # Define features and target\n",
    "# X = train_df.drop(columns=['id', 'Age'])\n",
    "# y = train_df['Age']\n",
    "# X_test = test_df.drop(columns=['id'])\n",
    "\n",
    "# # Create 'Age_Group' feature based on training data for consistency\n",
    "# def create_age_group(df):\n",
    "#     return pd.cut(df['Age'], bins=[0, 12, 18, 35, 60, 100], labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "# # Add 'Age_Group' feature to training data\n",
    "# X['Age_Group'] = create_age_group(train_df)\n",
    "\n",
    "# # Handle missing 'Age' in test data\n",
    "# test_df['Age'] = np.nan\n",
    "# # Add 'Age_Group' feature to test data\n",
    "# X_test['Age_Group'] = create_age_group(test_df)\n",
    "\n",
    "# # Define categorical and numerical columns\n",
    "# categorical_cols = ['Sex']\n",
    "# numerical_cols = [col for col in X.columns if col not in categorical_cols + ['Age_Group']]\n",
    "\n",
    "# # Define specific columns for discretization\n",
    "# discretized_cols = ['Length', 'Weight']\n",
    "\n",
    "# # Updated numerical preprocessing pipeline with discretization for specific columns\n",
    "# num_pipeline = Pipeline([\n",
    "#     ('imputer', SimpleImputer(strategy='mean')),\n",
    "#     ('discretizer', KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')),\n",
    "#     ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "#     ('log', FunctionTransformer(lambda x: np.log1p(x), validate=False)),\n",
    "#     ('scaler', StandardScaler())\n",
    "# ])\n",
    "\n",
    "# # Updated categorical preprocessing pipeline\n",
    "# cat_pipeline = Pipeline([\n",
    "#     ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#     ('encoder', OneHotEncoder(handle_unknown='ignore', drop='first'))\n",
    "# ])\n",
    "\n",
    "# # Combine all features\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', num_pipeline, [col for col in numerical_cols if col in discretized_cols]),\n",
    "#         ('cat', cat_pipeline, categorical_cols),\n",
    "#         ('age_group', OneHotEncoder(handle_unknown='ignore', drop='first'), ['Age_Group']),\n",
    "#     ])\n",
    "\n",
    "# # Define base models\n",
    "# base_models = [\n",
    "#     ('ridge', Ridge(alpha=1.0)),\n",
    "#     ('lasso', Lasso(alpha=0.01)),\n",
    "#     ('ths', TheilSenRegressor(fit_intercept=True, random_state=42))\n",
    "# ]\n",
    "\n",
    "# # Define the meta-model\n",
    "# meta_model = HuberRegressor()\n",
    "\n",
    "# # Stacking Regressor\n",
    "# stacking_regressor = StackingRegressor(\n",
    "#     estimators=base_models,\n",
    "#     final_estimator=meta_model,\n",
    "#     cv=5,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "# # Full pipeline with stacking\n",
    "# pipeline = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('stacking', stacking_regressor)\n",
    "# ])\n",
    "\n",
    "# # Cross-validation to evaluate performance\n",
    "# cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# cv_scores = cross_val_score(pipeline, X, y, cv=cv, scoring='neg_mean_absolute_error')\n",
    "# mean_cv_mae = -np.mean(cv_scores)\n",
    "# print(f'Stacking Cross-validated Mean Absolute Error (MAE): {mean_cv_mae:.4f}')\n",
    "\n",
    "# # Fit the model on the entire training data\n",
    "# pipeline.fit(X, y)\n",
    "\n",
    "# # Predict on the test set\n",
    "# test_predictions = pipeline.predict(X_test)\n",
    "\n",
    "# # Create submission file\n",
    "# submission5 = pd.DataFrame({\n",
    "#     'id': test_df['id'],\n",
    "#     'Age': test_predictions\n",
    "# })\n",
    "# submission5.to_csv('submissionffff.csv', index=False)\n",
    "\n",
    "# print(\"Submission file created: submission4.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission5.to_csv('submissionffff.csv', index=False)\n",
    "\n",
    "# print(\"Submission file created: submission4.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.linear_model import Ridge, Lasso, HuberRegressor, TheilSenRegressor\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder, FunctionTransformer, KBinsDiscretizer, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.metrics import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # import pandas as pd\n",
    "# # import numpy as np\n",
    "# # from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "# # from sklearn.linear_model import Ridge, Lasso, HuberRegressor, TheilSenRegressor\n",
    "# # from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder, FunctionTransformer, KBinsDiscretizer, RobustScaler\n",
    "# # from sklearn.impute import SimpleImputer\n",
    "# # from sklearn.pipeline import Pipeline\n",
    "# # from sklearn.compose import ColumnTransformer\n",
    "# # from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "\n",
    "# # Load the datasets\n",
    "# train_df = pd.read_csv('train.csv')\n",
    "# test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# # Define the feature engineering function\n",
    "# def feature_funk(df):\n",
    "#     df['Surface Area'] = 2 * (df['Length'] * df['Diameter'] + df['Length'] * df['Height'] + df['Diameter'] * df['Height'])\n",
    "#     df['Volume'] = df['Length'] * df['Diameter'] * df['Height']\n",
    "#     df['Log Weight'] = df['Weight'].apply(lambda x: np.log(x + 1) if x > 0 else np.nan)\n",
    "#     df['Length Bins'] = pd.qcut(df['Length'], q=4, labels=False)\n",
    "\n",
    "#     sex = {'I': 1, 'F': 3, 'M': 2}\n",
    "#     df['Sex'] = df['Sex'].replace(sex)\n",
    "#     df = df[df['Sex'].isin([1, 2, 3])]\n",
    "#     df['Sex'] = pd.to_numeric(df['Sex'])\n",
    "#     return df\n",
    "\n",
    "# # Apply the feature engineering function to both datasets\n",
    "# train_df = feature_funk(train_df)\n",
    "# test_df = feature_funk(test_df)\n",
    "\n",
    "# # Define features and target\n",
    "# X = train_df.drop(columns=['id', 'Age'])\n",
    "# y = train_df['Age']\n",
    "# X_test = test_df.drop(columns=['id'])\n",
    "\n",
    "# # Create 'Age_Group' feature based on training data for consistency\n",
    "# def create_age_group(df):\n",
    "#     return pd.cut(df['Age'], bins=[0, 12, 18, 35, 60, 100], labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "# # Add 'Age_Group' feature to training data\n",
    "# X['Age_Group'] = create_age_group(train_df)\n",
    "\n",
    "# # Handle missing 'Age' in test data\n",
    "# test_df['Age'] = np.nan\n",
    "# # Add 'Age_Group' feature to test data\n",
    "# X_test['Age_Group'] = create_age_group(test_df)\n",
    "\n",
    "# # Define categorical and numerical columns\n",
    "# categorical_cols = ['Sex']\n",
    "# numerical_cols = [col for col in X.columns if col not in categorical_cols + ['Age_Group']]\n",
    "\n",
    "# # Define specific columns for discretization\n",
    "# discretized_cols = ['Length', 'Weight']\n",
    "\n",
    "# # Updated numerical preprocessing pipeline with RobustScaler and discretization for specific columns\n",
    "# num_pipeline = Pipeline([\n",
    "#     ('imputer', SimpleImputer(strategy='mean')),\n",
    "#     ('discretizer', KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')),\n",
    "#     ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "#     ('log', FunctionTransformer(lambda x: np.log1p(x), validate=False)),\n",
    "#     ('scaler', RobustScaler())\n",
    "# ])\n",
    "\n",
    "# # Updated categorical preprocessing pipeline\n",
    "# cat_pipeline = Pipeline([\n",
    "#     ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#     ('encoder', OneHotEncoder(handle_unknown='ignore', drop='first'))\n",
    "# ])\n",
    "\n",
    "# # Combine all features\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', num_pipeline, [col for col in numerical_cols if col in discretized_cols]),\n",
    "#         ('cat', cat_pipeline, categorical_cols),\n",
    "#         ('age_group', OneHotEncoder(handle_unknown='ignore', drop='first'), ['Age_Group']),\n",
    "#     ])\n",
    "\n",
    "# # Define base models\n",
    "# base_models = [\n",
    "#     ('ridge', Ridge(alpha=1.0)),\n",
    "#     ('lasso', Lasso(alpha=0.01)),\n",
    "#     ('ths', TheilSenRegressor(fit_intercept=True, random_state=42))\n",
    "# ]\n",
    "\n",
    "# # Define the meta-model\n",
    "# meta_model = HuberRegressor()\n",
    "\n",
    "# # Stacking Regressor\n",
    "# stacking_regressor = StackingRegressor(\n",
    "#     estimators=base_models,\n",
    "#     final_estimator=meta_model,\n",
    "#     cv=5,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "# # Full pipeline with stacking\n",
    "# pipeline = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('stacking', stacking_regressor)\n",
    "# ])\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "\n",
    "\n",
    "# # Cross-validation to evaluate performance\n",
    "# cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# cv_scores = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring='neg_mean_absolute_error')\n",
    "# mean_cv_mae = -np.mean(cv_scores)\n",
    "# print(f'Stacking Cross-validated Mean Absolute Error (MAE): {mean_cv_mae:.4f}')\n",
    "\n",
    "# # Fit the model on the entire training data\n",
    "# pipeline.fit(X_train, y_train)\n",
    "\n",
    "# # Predict on the test set\n",
    "# test_predictions = pipeline.predict(X_test)\n",
    "\n",
    "# MAE = mean_squared_error(test_predictions,y_test)\n",
    "# print(f\"MAE: {MAE}\")\n",
    "\n",
    "\n",
    "#   df['Sex'] = df['Sex'].replace(sex)\n",
    "# Stacking Cross-validated Mean Absolute Error (MAE): 1.0175\n",
    "# MAE: 1.66882887899999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "# submission5 = pd.DataFrame({\n",
    "#     'id': test_df['id'],\n",
    "#     'Age': test_predictions\n",
    "# })\n",
    "# submission5.to_csv('submissionffff.csv', index=False)\n",
    "\n",
    "# print(\"Submission file created: submissionffff.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_2296\\2333839325.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Sex'] = pd.to_numeric(df['Sex'])\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_2296\\2333839325.py:24: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['Sex'] = df['Sex'].replace(sex)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Cross-validated Mean Absolute Error (MAE): 1.0196\n",
      "Submission file created: submissiontest.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:242: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "# from sklearn.linear_model import Ridge, Lasso, HuberRegressor, TheilSenRegressor\n",
    "# from sklearn.preprocessing import PolynomialFeatures, OneHotEncoder, FunctionTransformer, KBinsDiscretizer, RobustScaler\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.ensemble import StackingRegressor\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# # Load the datasets\n",
    "# train_df = pd.read_csv('train.csv')\n",
    "# test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# # Define the feature engineering function\n",
    "# def feature_funk(df):\n",
    "#     df['Surface Area'] = 2 * (df['Length'] * df['Diameter'] + df['Length'] * df['Height'] + df['Diameter'] * df['Height'])\n",
    "#     df['Volume'] = df['Length'] * df['Diameter'] * df['Height']\n",
    "#     df['Log Weight'] = df['Weight'].apply(lambda x: np.log(x + 1) if x > 0 else np.nan)\n",
    "#     df['Length Bins'] = pd.qcut(df['Length'], q=4, labels=False)\n",
    "\n",
    "#     sex = {'I': 1, 'F': 3, 'M': 2}\n",
    "#     df['Sex'] = df['Sex'].replace(sex)\n",
    "#     df = df[df['Sex'].isin([1, 2, 3])]\n",
    "#     df['Sex'] = pd.to_numeric(df['Sex'])\n",
    "#     return df\n",
    "\n",
    "# # Apply the feature engineering function to both datasets\n",
    "# train_df = feature_funk(train_df)\n",
    "# test_df = feature_funk(test_df)\n",
    "\n",
    "# # Define features and target\n",
    "# X = train_df.drop(columns=['id', 'Age'])\n",
    "# y = train_df['Age']\n",
    "# X_test_original = test_df.drop(columns=['id'])\n",
    "\n",
    "# # Create 'Age_Group' feature based on training data for consistency\n",
    "# def create_age_group(df):\n",
    "#     return pd.cut(df['Age'], bins=[0, 12, 18, 35, 60, 100], labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "# # Add 'Age_Group' feature to training data\n",
    "# X['Age_Group'] = create_age_group(train_df)\n",
    "\n",
    "# # Handle missing 'Age' in test data\n",
    "# test_df['Age'] = np.nan\n",
    "# # Add 'Age_Group' feature to test data\n",
    "# X_test_original['Age_Group'] = create_age_group(test_df)\n",
    "\n",
    "# # Define categorical and numerical columns\n",
    "# categorical_cols = ['Sex']\n",
    "# numerical_cols = [col for col in X.columns if col not in categorical_cols + ['Age_Group']]\n",
    "\n",
    "# # Define specific columns for discretization\n",
    "# discretized_cols = ['Length', 'Weight']\n",
    "\n",
    "# # Updated numerical preprocessing pipeline with RobustScaler and discretization for specific columns\n",
    "# num_pipeline = Pipeline([\n",
    "#     ('imputer', SimpleImputer(strategy='mean')),\n",
    "#     ('discretizer', KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')),\n",
    "#     ('poly', PolynomialFeatures(degree=2, include_bias=False)),  # You can lower the degree if necessary\n",
    "#     ('log', FunctionTransformer(lambda x: np.log1p(x), validate=False)),\n",
    "#     ('scaler', RobustScaler())\n",
    "# ])\n",
    "\n",
    "# # Updated categorical preprocessing pipeline\n",
    "# cat_pipeline = Pipeline([\n",
    "#     ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#     ('encoder', OneHotEncoder(handle_unknown='ignore', drop='first'))\n",
    "# ])\n",
    "\n",
    "# # Combine all features\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', num_pipeline, [col for col in numerical_cols if col in discretized_cols]),\n",
    "#         ('cat', cat_pipeline, categorical_cols),\n",
    "#         ('age_group', OneHotEncoder(handle_unknown='ignore', drop='first'), ['Age_Group']),\n",
    "#     ])\n",
    "\n",
    "# # Define base models\n",
    "# base_models = [\n",
    "#     ('ridge', Ridge(alpha=10.0)),  # Increased alpha for more regularization\n",
    "#     ('lasso', Lasso(alpha=0.1)),  # Adjust alpha\n",
    "#     ('ths', TheilSenRegressor(fit_intercept=True, random_state=42))\n",
    "# ]\n",
    "\n",
    "# # Define the meta-model\n",
    "# meta_model = HuberRegressor()\n",
    "\n",
    "# # Stacking Regressor\n",
    "# stacking_regressor = StackingRegressor(\n",
    "#     estimators=base_models,\n",
    "#     final_estimator=meta_model,\n",
    "#     cv=5,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "# # Full pipeline with stacking\n",
    "# pipeline = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('stacking', stacking_regressor)\n",
    "# ])\n",
    "\n",
    "# # Split the training data\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# # Cross-validation to evaluate performance\n",
    "# cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# cv_scores = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring='neg_mean_absolute_error')\n",
    "# mean_cv_mae = -np.mean(cv_scores)\n",
    "# print(f'Stacking Cross-validated Mean Absolute Error (MAE): {mean_cv_mae:.4f}')\n",
    "\n",
    "# # Fit the model on the entire training data\n",
    "# pipeline.fit(X, y)\n",
    "\n",
    "# # Predict on the validation set (used for MAE calculation here)\n",
    "# # val_predictions = pipeline.predict(X_val)\n",
    "\n",
    "# # # Calculate MAE for validation set\n",
    "# # MAE = mean_absolute_error(y_val, val_predictions)\n",
    "# # print(f\"Validation MAE: {MAE:.4f}\")\n",
    "\n",
    "# # Fit the model on the entire training data and predict on the actual test set\n",
    "# # pipeline.fit(X, y)\n",
    "# test_predictions = pipeline.predict(X_test_original)\n",
    "\n",
    "# # Create submission file\n",
    "# submission5 = pd.DataFrame({\n",
    "#     'id': test_df['id'],\n",
    "#     'Age': test_predictions\n",
    "# })\n",
    "# submission5.to_csv('submissiontest.csv', index=False)\n",
    "\n",
    "# print(\"Submission file created: submissiontest.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_2296\\3187486693.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Sex'] = pd.to_numeric(df['Sex'])\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_2296\\3187486693.py:23: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['Sex'] = df['Sex'].replace(sex)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 10 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 21 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 31 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 40 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 48 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 55 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 61 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 66 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 70 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 73 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 75 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 76 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 10 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 21 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 31 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 40 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 48 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 55 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 61 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 66 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 70 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 73 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 75 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 76 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 10 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 21 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 31 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 40 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 48 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 55 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 61 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 66 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 70 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 73 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 75 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 76 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 89\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Cross-validation to evaluate performance\u001b[39;00m\n\u001b[0;32m     88\u001b[0m cv \u001b[38;5;241m=\u001b[39m KFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 89\u001b[0m cv_scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mneg_mean_absolute_error\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m mean_cv_mae \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mmean(cv_scores)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStacking Cross-validated Mean Absolute Error (MAE): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_cv_mae\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:712\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    710\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 712\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    725\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:423\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    422\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 423\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:888\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    886\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    887\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 888\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    891\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    892\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:473\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    472\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m routed_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 473\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_stacking.py:971\u001b[0m, in \u001b[0;36mStackingRegressor.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    969\u001b[0m _raise_for_unsupported_routing(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n\u001b[0;32m    970\u001b[0m y \u001b[38;5;241m=\u001b[39m column_or_1d(y, warn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 971\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_stacking.py:264\u001b[0m, in \u001b[0;36m_BaseStacking.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(cv, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m cv\u001b[38;5;241m.\u001b[39mrandom_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    262\u001b[0m         cv\u001b[38;5;241m.\u001b[39mrandom_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mRandomState()\n\u001b[1;32m--> 264\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcross_val_predict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m            \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mall_estimators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack_method_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdrop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;66;03m# Only not None or not 'drop' estimators will be used in transform.\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;66;03m# Remove the None from the method as well.\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    282\u001b[0m     meth\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (meth, est) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_, all_estimators)\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    285\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import Ridge, Lasso, HuberRegressor, TheilSenRegressor\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder, FunctionTransformer, KBinsDiscretizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Define the feature engineering function\n",
    "def feature_funk(df):\n",
    "    df['Surface Area'] = 2 * (df['Length'] * df['Diameter'] + df['Length'] * df['Height'] + df['Diameter'] * df['Height'])\n",
    "    df['Volume'] = df['Length'] * df['Diameter'] * df['Height']\n",
    "    df['Log Weight'] = df['Weight'].apply(lambda x: np.log(x + 1) if x > 0 else np.nan)\n",
    "    df['Length Bins'] = pd.qcut(df['Length'], q=4, labels=False)\n",
    "\n",
    "    sex = {'I': 1, 'F': 3, 'M': 2}\n",
    "    df['Sex'] = df['Sex'].replace(sex)\n",
    "    df = df[df['Sex'].isin([1, 2, 3])]\n",
    "    df['Sex'] = pd.to_numeric(df['Sex'])\n",
    "    return df\n",
    "\n",
    "# Apply the feature engineering function to both datasets\n",
    "train_df = feature_funk(train_df)\n",
    "test_df = feature_funk(test_df)\n",
    "\n",
    "# Define features and target\n",
    "X = train_df.drop(columns=['id', 'Age'])\n",
    "y = train_df['Age']\n",
    "X_test = test_df.drop(columns=['id'])\n",
    "\n",
    "# Define categorical and numerical columns\n",
    "categorical_cols = ['Sex']\n",
    "numerical_cols = [col for col in X.columns if col not in categorical_cols]\n",
    "\n",
    "# Updated numerical preprocessing pipeline with feature engineering\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('log', FunctionTransformer(lambda x: np.log1p(x), validate=False)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('discretizer', KBinsDiscretizer(n_bins=4, encode='onehot-dense', strategy='quantile'))\n",
    "])\n",
    "\n",
    "# Updated categorical preprocessing pipeline\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', drop='first'))\n",
    "])\n",
    "\n",
    "# Combine all features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_pipeline, numerical_cols),\n",
    "        ('cat', cat_pipeline, categorical_cols),\n",
    "    ])\n",
    "\n",
    "# Define base models\n",
    "base_models = [\n",
    "    ('ridge', Ridge(alpha=1.0)),\n",
    "    ('lasso', Lasso(alpha=0.01)),\n",
    "    ('ths', TheilSenRegressor(fit_intercept=True, random_state=42))\n",
    "]\n",
    "\n",
    "# Define the meta-model\n",
    "meta_model = HuberRegressor()\n",
    "\n",
    "# Stacking Regressor\n",
    "stacking_regressor = StackingRegressor(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Full pipeline with stacking\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('stacking', stacking_regressor)\n",
    "])\n",
    "\n",
    "# Cross-validation to evaluate performance\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(pipeline, X, y, cv=cv, scoring='neg_mean_absolute_error')\n",
    "mean_cv_mae = -np.mean(cv_scores)\n",
    "print(f'Stacking Cross-validated Mean Absolute Error (MAE): {mean_cv_mae:.4f}')\n",
    "\n",
    "# Fit the model on the entire training data\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# Predict on the test set\n",
    "test_predictions = pipeline.predict(X_test)\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'Age': test_predictions\n",
    "})\n",
    "submission.to_csv('submission11.csv', index=False)\n",
    "\n",
    "print(\"Submission file created: submission11.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
