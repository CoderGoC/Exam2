{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures\n",
    "# from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', drop='first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Load the datasets\n",
    "# train_df = pd.read_csv('train.csv')\n",
    "# test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# # Define features and target\n",
    "# X = train_df.drop(columns=['id', 'Age'])\n",
    "# y = train_df['Age']\n",
    "# X_test = test_df.drop(columns=['id'])\n",
    "\n",
    "# # Define categorical and numerical columns\n",
    "# categorical_cols = ['Sex']\n",
    "# numerical_cols = X.columns.difference(categorical_cols)\n",
    "\n",
    "# # Preprocessing pipeline\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', Pipeline([\n",
    "#             ('imputer', SimpleImputer(strategy='median')),\n",
    "#             ('poly', PolynomialFeatures(degree=4, include_bias=False, interaction_only=True)),\n",
    "#             ('scaler', StandardScaler())\n",
    "#         ]), numerical_cols),\n",
    "#         ('cat', Pipeline([\n",
    "#             ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#             ('encoder', OneHotEncoder(handle_unknown='ignore', drop='first'))\n",
    "#         ]), categorical_cols)\n",
    "#     ])\n",
    "\n",
    "# # Model\n",
    "# model = HuberRegressor(alpha=0.1)\n",
    "\n",
    "# # Pipeline\n",
    "# pipeline = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('model', model)\n",
    "# ])\n",
    "\n",
    "# # Cross-validation\n",
    "# cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "# cv_scores = cross_val_score(pipeline, X, y, cv=cv, scoring='neg_mean_absolute_error')\n",
    "# mean_cv_mae = -np.mean(cv_scores)\n",
    "# print(f'Cross-validated Mean Absolute Error (MAE): {mean_cv_mae:.4f}')\n",
    "\n",
    "# # Grid search hyperparameters for HuberRegressor\n",
    "# param_grid = {\n",
    "#     'model__alpha': [0.01, 0.1, 1],\n",
    "#     'model__epsilon': [1.35, 1.5, 1.75],\n",
    "#     'model__max_iter': [100, 500, 1000],\n",
    "# }\n",
    "\n",
    "# # Grid Search CV\n",
    "# grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "# grid_search.fit(X, y)\n",
    "\n",
    "# # Best parameters and best score\n",
    "# print(f'Best parameters found: {grid_search.best_params_}')\n",
    "# print(f'Best cross-validated MAE: {-grid_search.best_score_:.4f}')\n",
    "\n",
    "# # Final model with the best parameters\n",
    "# best_model = grid_search.best_estimator_\n",
    "# best_model.fit(X, y)\n",
    "\n",
    "# # Predict on test set\n",
    "# test_predictions = best_model.predict(X_test)\n",
    "\n",
    "# # Create submission file\n",
    "# submission1 = pd.DataFrame({\n",
    "#     'id': test_df['id'],\n",
    "#     'Age': test_predictions\n",
    "# })\n",
    "\n",
    "# submission1.to_csv('submission.csv', index=False)\n",
    "\n",
    "# print(\"Submission file created: submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# train_df = pd.read_csv('train.csv')\n",
    "# test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# X = train_df.drop(columns=['id', 'Age'])\n",
    "# y = train_df['Age']\n",
    "# X_test = test_df.drop(columns=['id'])\n",
    "\n",
    "\n",
    "# categorical_cols = ['Sex']\n",
    "# numerical_cols = X.columns.difference(categorical_cols)\n",
    "\n",
    "\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', Pipeline([\n",
    "#             ('imputer', SimpleImputer(strategy='mean')),\n",
    "#             ('poly', PolynomialFeatures(degree=3, include_bias=False, interaction_only=True)),\n",
    "#             ('scaler', StandardScaler())\n",
    "#         ]), numerical_cols),\n",
    "#         ('cat', Pipeline([\n",
    "#             ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#             ('encoder', OneHotEncoder(handle_unknown='ignore', drop='first'))\n",
    "#         ]), categorical_cols)\n",
    "#     ])\n",
    "\n",
    "\n",
    "# model = HuberRegressor(alpha=0.1)\n",
    "\n",
    "\n",
    "# pipeline = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('model', model)\n",
    "# ])\n",
    "\n",
    "\n",
    "# cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# cv_scores = cross_val_score(pipeline, X, y, cv=cv, scoring='neg_mean_absolute_error')\n",
    "# mean_cv_mae = -np.mean(cv_scores)\n",
    "# print(f'Cross-validated Mean Absolute Error (MAE): {mean_cv_mae:.4f}')\n",
    "\n",
    "\n",
    "# param_grid = {\n",
    "#     'model__n_estimators': [100, 300, 400],\n",
    "#     'model__learning_rate': [0.01, 0.1, 1],\n",
    "#     'model__max_depth': [1, 5, 9],\n",
    "# }\n",
    "\n",
    "# grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "# grid_search.fit(X, y)\n",
    "\n",
    "# print(f'Best parameters found: {grid_search.best_params_}')\n",
    "# print(f'Best cross-validated MAE: {-grid_search.best_score_:.4f}')\n",
    "\n",
    "\n",
    "# best_model = grid_search.best_estimator_\n",
    "# best_model.fit(X, y)\n",
    "\n",
    "\n",
    "# test_predictions = best_model.predict(X_test)\n",
    "\n",
    "\n",
    "# submission1 = pd.DataFrame({\n",
    "#     'id': test_df['id'],\n",
    "#     'Age': test_predictions\n",
    "# })\n",
    "\n",
    "\n",
    "# submission1.to_csv('submission.csv', index=False)\n",
    "\n",
    "# print(\"Submission file created: submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import KFold, cross_val_score\n",
    "# from sklearn.linear_model import Ridge, Lasso, HuberRegressor, TheilSenRegressor\n",
    "# from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder, FunctionTransformer\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "# # Load the datasets\n",
    "# train_df = pd.read_csv('train.csv')\n",
    "# test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# # Define the feature engineering function\n",
    "# def feature_funk(df):\n",
    "#     df['Surface Area'] = 2 * (df['Length'] * df['Diameter'] + df['Length'] * df['Height'] + df['Diameter'] * df['Height'])\n",
    "#     df['Volume'] = df['Length'] * df['Diameter'] * df['Height']\n",
    "#     df['Log Weight'] = df['Weight'].apply(lambda x: np.log(x + 1) if x > 0 else np.nan)\n",
    "#     df['Length Bins'] = pd.qcut(df['Length'], q=4, labels=False)\n",
    "    \n",
    "#     sex = {'I': 1, 'F': 3, 'M': 2}\n",
    "#     df['Sex'] = df['Sex'].replace(sex)\n",
    "#     df = df[df['Sex'].isin([1, 2, 3])]\n",
    "#     df['Sex'] = pd.to_numeric(df['Sex'])\n",
    "#     return df\n",
    "\n",
    "# # Apply the feature engineering function\n",
    "# train_df = feature_funk(train_df)\n",
    "# test_df = feature_funk(test_df)\n",
    "\n",
    "# # Define features and target\n",
    "# X = train_df.drop(columns=['id', 'Age'])\n",
    "# y = train_df['Age']\n",
    "# X_test = test_df.drop(columns=['id'])\n",
    "\n",
    "# # Quantile filtering function\n",
    "# def filter_outliers(df, feature, quantile=0.95):\n",
    "#     threshold = df[feature].quantile(quantile)\n",
    "#     return df[df[feature] <= threshold]\n",
    "\n",
    "# # Apply quantile filtering to the original training data before splitting\n",
    "# filtered_train_df = filter_outliers(train_df, 'Age')\n",
    "\n",
    "# # Define features and target after filtering\n",
    "# X = filtered_train_df.drop(columns=['id', 'Age'])\n",
    "# y = filtered_train_df['Age']\n",
    "# X_test = test_df.drop(columns=['id'])\n",
    "\n",
    "# # Function to create age groups\n",
    "# def create_age_group(df):\n",
    "#     return pd.cut(df['Age'], bins=[0, 12, 18, 35, 60, 100], labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "# # Add 'Age_Group' feature to training data\n",
    "# X['Age_Group'] = create_age_group(filtered_train_df)\n",
    "\n",
    "# # Define categorical and numerical columns\n",
    "# categorical_cols = ['Sex']\n",
    "# numerical_cols = X.columns.difference(categorical_cols + ['Age_Group'])\n",
    "\n",
    "# # Updated numerical preprocessing pipeline with feature engineering\n",
    "# num_pipeline = Pipeline([\n",
    "#     ('imputer', SimpleImputer(strategy='mean')),\n",
    "#     ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "#     ('log', FunctionTransformer(lambda x: np.log1p(x), validate=False)),\n",
    "#     ('scaler', StandardScaler())\n",
    "# ])\n",
    "\n",
    "# # Updated categorical preprocessing pipeline\n",
    "# cat_pipeline = Pipeline([\n",
    "#     ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#     ('encoder', OneHotEncoder(handle_unknown='ignore', drop='first'))\n",
    "# ])\n",
    "\n",
    "# # Combine all features\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', num_pipeline, numerical_cols),\n",
    "#         ('cat', cat_pipeline, categorical_cols),\n",
    "#         ('age_group', OneHotEncoder(drop='first'), ['Age_Group']),\n",
    "#     ])\n",
    "\n",
    "# # Define base models\n",
    "# base_models = [\n",
    "#     ('ridge', Ridge(alpha=1.0)),\n",
    "#     ('lasso', Lasso(alpha=0.01)),\n",
    "#     ('ths', TheilSenRegressor(fit_intercept=True, random_state=42)),\n",
    "#     ('hub', HuberRegressor(epsilon=1.35, max_iter=100, alpha=0.1))\n",
    "# ]\n",
    "\n",
    "# # Define the meta-model\n",
    "# meta_model = HuberRegressor()\n",
    "\n",
    "# # Stacking Regressor\n",
    "# stacking_regressor = StackingRegressor(\n",
    "#     estimators=base_models,\n",
    "#     final_estimator=meta_model,\n",
    "#     cv=5,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "# # Full pipeline with stacking\n",
    "# pipeline = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('stacking', stacking_regressor)\n",
    "# ])\n",
    "\n",
    "# # Cross-validation to evaluate performance\n",
    "# cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# cv_scores = cross_val_score(pipeline, X, y, cv=cv, scoring='neg_mean_absolute_error')\n",
    "# mean_cv_mae = -np.mean(cv_scores)\n",
    "# print(f'Stacking Cross-validated Mean Absolute Error (MAE): {mean_cv_mae:.4f}')\n",
    "\n",
    "# # Fit the model on the entire training data\n",
    "# pipeline.fit(X, y)\n",
    "\n",
    "# # Predict on the test set\n",
    "# test_predictions = pipeline.predict(X_test)\n",
    "\n",
    "# # Create submission file\n",
    "# submission4 = pd.DataFrame({\n",
    "#     'id': test_df['id'],\n",
    "#     'Age': test_predictions\n",
    "# })\n",
    "# submission4.to_csv('submission.csv', index=False)\n",
    "\n",
    "# print(\"Submission file created: submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import KFold, cross_val_score\n",
    "# from sklearn.linear_model import Ridge, Lasso, HuberRegressor, TheilSenRegressor\n",
    "# from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder, FunctionTransformer\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "# # Load the datasets\n",
    "# train_df = pd.read_csv('train.csv')\n",
    "# test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# # Define the feature engineering function\n",
    "# def feature_funk(df):\n",
    "#     df['Surface Area'] = 2 * (df['Length'] * df['Diameter'] + df['Length'] * df['Height'] + df['Diameter'] * df['Height'])\n",
    "#     df['Volume'] = df['Length'] * df['Diameter'] * df['Height']\n",
    "#     df['Log Weight'] = df['Weight'].apply(lambda x: np.log(x + 1) if x > 0 else np.nan)\n",
    "#     df['Length Bins'] = pd.qcut(df['Length'], q=4, labels=False)\n",
    "\n",
    "#     # Convert 'Sex' to numeric\n",
    "#     sex = {'I': 1, 'F': 3, 'M': 2}\n",
    "#     df['Sex'] = df['Sex'].replace(sex)\n",
    "#     df = df[df['Sex'].isin([1, 2, 3])]  # Filter out invalid sex values\n",
    "#     df['Sex'] = pd.to_numeric(df['Sex'])\n",
    "#     return df\n",
    "\n",
    "# # Apply the feature engineering function to both datasets\n",
    "# train_df = feature_funk(train_df)\n",
    "# test_df = feature_funk(test_df)\n",
    "\n",
    "# # Define features and target\n",
    "# X = train_df.drop(columns=['id', 'Age'])\n",
    "# y = train_df['Age']\n",
    "# X_test = test_df.drop(columns=['id'])\n",
    "\n",
    "# # Create 'Age_Group' feature\n",
    "# def create_age_group(df):\n",
    "#     return pd.cut(df['Age'], bins=[0, 12, 18, 35, 60, 100], labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "# # Add 'Age_Group' feature to training and test data\n",
    "# X['Age_Group'] = create_age_group(train_df)\n",
    "# X_test['Age_Group'] = create_age_group(test_df.assign(Age=np.nan))\n",
    "\n",
    "# # Define categorical and numerical columns\n",
    "# categorical_cols = ['Sex']\n",
    "# numerical_cols = [col for col in X.columns if col not in categorical_cols + ['Age_Group']]\n",
    "\n",
    "# # Numerical preprocessing pipeline\n",
    "# num_pipeline = Pipeline([\n",
    "#     ('imputer', SimpleImputer(strategy='mean')),\n",
    "#     ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "#     ('log', FunctionTransformer(lambda x: np.log1p(x), validate=False)),\n",
    "#     ('scaler', StandardScaler())\n",
    "# ])\n",
    "\n",
    "# # Categorical preprocessing pipeline\n",
    "# cat_pipeline = Pipeline([\n",
    "#     ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#     ('encoder', OneHotEncoder(handle_unknown='ignore', drop='first'))\n",
    "# ])\n",
    "\n",
    "# # Combine all features\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', num_pipeline, numerical_cols),\n",
    "#         ('cat', cat_pipeline, categorical_cols),\n",
    "#         ('age_group', OneHotEncoder(drop='first'), ['Age_Group']),\n",
    "#     ])\n",
    "\n",
    "# # Define base models\n",
    "# base_models = [\n",
    "#     ('ridge', Ridge(alpha=1.0)),\n",
    "#     ('lasso', Lasso(alpha=0.01)),\n",
    "#     ('ths', TheilSenRegressor(fit_intercept=True, random_state=42)),\n",
    "#     ('hub', HuberRegressor(epsilon=1.35, max_iter=100, alpha=0.1))\n",
    "# ]\n",
    "\n",
    "# # Define the meta-model\n",
    "# meta_model = HuberRegressor()\n",
    "\n",
    "# # Stacking Regressor\n",
    "# stacking_regressor = StackingRegressor(\n",
    "#     estimators=base_models,\n",
    "#     final_estimator=meta_model,\n",
    "#     cv=5,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "# # Full pipeline with stacking\n",
    "# pipeline = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('stacking', stacking_regressor)\n",
    "# ])\n",
    "\n",
    "# # Cross-validation to evaluate performance\n",
    "# cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# cv_scores = cross_val_score(pipeline, X, y, cv=cv, scoring='neg_mean_absolute_error')\n",
    "# mean_cv_mae = -np.mean(cv_scores)\n",
    "# print(f'Stacking Cross-validated Mean Absolute Error (MAE): {mean_cv_mae:.4f}')\n",
    "\n",
    "# # Fit the model on the entire training data\n",
    "# pipeline.fit(X, y)\n",
    "\n",
    "# # Predict on the test set\n",
    "# test_predictions = pipeline.predict(X_test)\n",
    "\n",
    "# # Create submission file\n",
    "# submission2 = pd.DataFrame({\n",
    "#     'id': test_df['id'],\n",
    "#     'Age': test_predictions\n",
    "# })\n",
    "# submission2.to_csv('submission.csv', index=False)\n",
    "\n",
    "# print(\"Submission file created: submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import KFold, cross_val_score\n",
    "# from sklearn.linear_model import Ridge, Lasso, HuberRegressor, TheilSenRegressor\n",
    "# from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder, FunctionTransformer, KBinsDiscretizer\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "# # Load the datasets\n",
    "# train_df = pd.read_csv('train.csv')\n",
    "# test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# # Define the feature engineering function\n",
    "# def feature_funk(df):\n",
    "#     df['Surface Area'] = 2 * (df['Length'] * df['Diameter'] + df['Length'] * df['Height'] + df['Diameter'] * df['Height'])\n",
    "#     df['Volume'] = df['Length'] * df['Diameter'] * df['Height']\n",
    "#     df['Log Weight'] = df['Weight'].apply(lambda x: np.log(x + 1) if x > 0 else np.nan)\n",
    "#     df['Length Bins'] = pd.qcut(df['Length'], q=4, labels=False)\n",
    "\n",
    "#     sex = {'I': 1, 'F': 3, 'M': 2}\n",
    "#     df['Sex'] = df['Sex'].replace(sex)\n",
    "#     df = df[df['Sex'].isin([1, 2, 3])]\n",
    "#     df['Sex'] = pd.to_numeric(df['Sex'])\n",
    "#     return df\n",
    "\n",
    "# # Apply the feature engineering function to both datasets\n",
    "# train_df = feature_funk(train_df)\n",
    "# test_df = feature_funk(test_df)\n",
    "\n",
    "# # Define features and target\n",
    "# X = train_df.drop(columns=['id', 'Age'])\n",
    "# y = train_df['Age']\n",
    "# X_test = test_df.drop(columns=['id'])\n",
    "\n",
    "# # Define categorical and numerical columns\n",
    "# categorical_cols = ['Sex']\n",
    "# numerical_cols = [col for col in X.columns if col not in categorical_cols]\n",
    "\n",
    "# # Define specific columns for discretization\n",
    "# discretized_cols = ['Length', 'Weight']\n",
    "\n",
    "# # Updated numerical preprocessing pipeline with discretization for specific columns\n",
    "# num_pipeline = Pipeline([\n",
    "#     ('imputer', SimpleImputer(strategy='mean')),\n",
    "#     ('discretizer', KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')),\n",
    "#     ('poly', PolynomialFeatures(degree=3, include_bias=False)),\n",
    "#     ('log', FunctionTransformer(lambda x: np.log1p(x), validate=False)),\n",
    "#     ('scaler', StandardScaler())\n",
    "# ])\n",
    "\n",
    "# # Updated categorical preprocessing pipeline\n",
    "# cat_pipeline = Pipeline([\n",
    "#     ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#     ('encoder', OneHotEncoder(handle_unknown='ignore', drop='first'))\n",
    "# ])\n",
    "\n",
    "# # Combine all features\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', num_pipeline, [col for col in numerical_cols if col in discretized_cols]),\n",
    "#         ('cat', cat_pipeline, categorical_cols)\n",
    "#     ])\n",
    "\n",
    "# # Define base models\n",
    "# base_models = [\n",
    "#     ('ridge', Ridge(alpha=1.0)),\n",
    "#     ('lasso', Lasso(alpha=0.01)),\n",
    "#     ('ths', TheilSenRegressor(fit_intercept=True, random_state=42))\n",
    "# ]\n",
    "\n",
    "# # Define the meta-model\n",
    "# meta_model = HuberRegressor()\n",
    "\n",
    "# # Stacking Regressor\n",
    "# stacking_regressor = StackingRegressor(\n",
    "#     estimators=base_models,\n",
    "#     final_estimator=meta_model,\n",
    "#     cv=5,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "# # Full pipeline with stacking\n",
    "# pipeline = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('stacking', stacking_regressor)\n",
    "# ])\n",
    "\n",
    "# # Cross-validation to evaluate performance\n",
    "# cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# cv_scores = cross_val_score(pipeline, X, y, cv=cv, scoring='neg_mean_absolute_error')\n",
    "# mean_cv_mae = -np.mean(cv_scores)\n",
    "# print(f'Stacking Cross-validated Mean Absolute Error (MAE): {mean_cv_mae:.4f}')\n",
    "\n",
    "# # Fit the model on the entire training data\n",
    "# pipeline.fit(X, y)\n",
    "\n",
    "# # Predict on the test set\n",
    "# test_predictions = pipeline.predict(X_test)\n",
    "\n",
    "# # Create submission file\n",
    "# submission5 = pd.DataFrame({\n",
    "#     'id': test_df['id'],\n",
    "#     'Age': test_predictions\n",
    "# })\n",
    "# submission5.to_csv('submission3.csv', index=False)\n",
    "\n",
    "# print(\"Submission file created: submission3.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
    "# from sklearn.preprocessing import OneHotEncoder, MaxAbsScaler, KBinsDiscretizer\n",
    "# from sklearn.linear_model import HuberRegressor, Ridge, Lasso, LinearRegression\n",
    "# from sklearn.ensemble import StackingRegressor\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# # Load the datasets\n",
    "# train_df = pd.read_csv('train.csv')\n",
    "# test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# # Define the target and features\n",
    "# X = train_df.drop(columns=['id', 'Age'])\n",
    "# y = train_df['Age']\n",
    "# X_test = test_df.drop(columns=['id'])\n",
    "\n",
    "# # Categorical and numerical columns\n",
    "# categorical_cols = ['Sex']\n",
    "# numerical_cols = X.columns.difference(categorical_cols)\n",
    "\n",
    "# # Preprocessing pipeline\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', Pipeline([\n",
    "#             ('imputer', SimpleImputer(strategy='mean')),\n",
    "#             ('binning', KBinsDiscretizer(n_bins=10, encode='onehot', strategy='uniform')),\n",
    "#             ('scaler', MaxAbsScaler())  # Use MaxAbsScaler for sparse matrix compatibility\n",
    "#         ]), numerical_cols),\n",
    "#         ('cat', Pipeline([\n",
    "#             ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#             ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # Dense output for compatibility\n",
    "#         ]), categorical_cols)\n",
    "#     ])\n",
    "\n",
    "# # Model selection: Add HuberRegressor and Ridge\n",
    "# base_models = [\n",
    "#     ('lasso', Lasso()),\n",
    "#     ('ridge', Ridge()),\n",
    "#     ('linear', LinearRegression())\n",
    "# ]\n",
    "\n",
    "# stacked_model = StackingRegressor(\n",
    "#     estimators=base_models,\n",
    "#     final_estimator=HuberRegressor()\n",
    "# )\n",
    "\n",
    "# # Create the full pipeline with StackingRegressor\n",
    "# pipeline = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('model', stacked_model)\n",
    "# ])\n",
    "\n",
    "# # Use cross-validation to assess the model's performance\n",
    "# cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# cv_scores = cross_val_score(pipeline, X, y, cv=cv, scoring='neg_mean_absolute_error')\n",
    "# mean_cv_mae = -np.mean(cv_scores)\n",
    "# print(f'Cross-validated Mean Absolute Error (MAE): {mean_cv_mae:.4f}')\n",
    "\n",
    "# # Hyperparameter tuning using RandomizedSearchCV\n",
    "# param_distributions = {\n",
    "#     'model__final_estimator__epsilon': [1.1, 1.2, 1.5],  # Parameters for HuberRegressor\n",
    "#     'model__lasso__alpha': [0.01, 0.1, 1.0],  # Parameters for Lasso\n",
    "#     'model__ridge__alpha': [0.01, 0.1, 1.0]  # Parameters for Ridge\n",
    "# }\n",
    "\n",
    "# random_search = RandomizedSearchCV(pipeline, param_distributions, n_iter=50, cv=cv, scoring='neg_mean_absolute_error', n_jobs=-1, random_state=42)\n",
    "# random_search.fit(X, y)\n",
    "\n",
    "# print(f'Best parameters found: {random_search.best_params_}')\n",
    "# print(f'Best cross-validated MAE: {-random_search.best_score_:.4f}')\n",
    "\n",
    "# # Train the model on the entire training set using the best parameters\n",
    "# best_model = random_search.best_estimator_\n",
    "# best_model.fit(X, y)\n",
    "\n",
    "# # Predict on the test set\n",
    "# test_predictions = best_model.predict(X_test)\n",
    "\n",
    "# # Prepare the submission file\n",
    "# submission = pd.DataFrame({\n",
    "#     'id': test_df['id'],\n",
    "#     'Age': test_predictions\n",
    "# })\n",
    "\n",
    "# # Save the submission file\n",
    "# submission.to_csv('submissionnew.csv', index=False)\n",
    "\n",
    "# print(\"Submission file created: submissionnew.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from xgboost) (2.0.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from xgboost) (1.14.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import xgboost as xgb\n",
    "# from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "# from sklearn.preprocessing import OneHotEncoder, StandardScaler, KBinsDiscretizer\n",
    "# from sklearn.linear_model import HuberRegressor, Ridge, Lasso, LinearRegression\n",
    "# from sklearn.ensemble import GradientBoostingRegressor, StackingRegressor\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# # Load the datasets\n",
    "# train_df = pd.read_csv('train.csv')\n",
    "# test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# # Define the target and features\n",
    "# X = train_df.drop(columns=['id', 'Age'])\n",
    "# y = train_df['Age']\n",
    "# X_test = test_df.drop(columns=['id'])\n",
    "\n",
    "# # Categorical and numerical columns\n",
    "# categorical_cols = ['Sex']\n",
    "# numerical_cols = X.columns.difference(categorical_cols)\n",
    "\n",
    "# # Preprocessing pipeline\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', Pipeline([\n",
    "#             ('imputer', SimpleImputer(strategy='mean')),\n",
    "#             ('scaler', StandardScaler())  # StandardScaler to normalize features\n",
    "#         ]), numerical_cols),\n",
    "#         ('cat', Pipeline([\n",
    "#             ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#             ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # Dense output for compatibility\n",
    "#         ]), categorical_cols)\n",
    "#     ])\n",
    "\n",
    "# # Model selection\n",
    "# base_models = [\n",
    "#     ('lasso', Lasso()),\n",
    "#     ('ridge', Ridge()),\n",
    "#     ('linear', LinearRegression()),\n",
    "#     ('xgb', xgb.XGBRegressor())  # Include XGBoost Regressor as a base model\n",
    "# ]\n",
    "\n",
    "# stacked_model = StackingRegressor(\n",
    "#     estimators=base_models,\n",
    "#     final_estimator=HuberRegressor()\n",
    "# )\n",
    "\n",
    "# # Create the full pipeline with StackingRegressor\n",
    "# pipeline = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('model', stacked_model)\n",
    "# ])\n",
    "\n",
    "# # Use cross-validation to assess the model's performance\n",
    "# cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# cv_scores = cross_val_score(pipeline, X, y, cv=cv, scoring='neg_mean_absolute_error')\n",
    "# mean_cv_mae = -np.mean(cv_scores)\n",
    "# print(f'Cross-validated Mean Absolute Error (MAE): {mean_cv_mae:.4f}')\n",
    "\n",
    "# # Hyperparameter tuning using GridSearchCV\n",
    "# param_grid = {\n",
    "#     'model__final_estimator__epsilon': [1.1, 1.2, 1.5],\n",
    "#     'model__ridge__alpha': [0.01, 0.1, 1.0],\n",
    "#     'model__lasso__alpha': [0.01, 0.1, 1.0],\n",
    "#     'model__xgb__n_estimators': [100, 200, 300],\n",
    "#     'model__xgb__learning_rate': [0.01, 0.1, 0.2],\n",
    "#     'model__xgb__max_depth': [3, 5, 7]\n",
    "# }\n",
    "\n",
    "# grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "# grid_search.fit(X, y)\n",
    "\n",
    "# print(f'Best parameters found: {grid_search.best_params_}')\n",
    "# print(f'Best cross-validated MAE: {-grid_search.best_score_:.4f}')\n",
    "\n",
    "# # Train the model on the entire training set using the best parameters\n",
    "# best_model = grid_search.best_estimator_\n",
    "# best_model.fit(X, y)\n",
    "\n",
    "# # Predict on the test set\n",
    "# test_predictions = best_model.predict(X_test)\n",
    "\n",
    "# # Prepare the submission file\n",
    "# submission = pd.DataFrame({\n",
    "#     'id': test_df['id'],\n",
    "#     'Age': test_predictions\n",
    "# })\n",
    "\n",
    "# # Save the submission file\n",
    "# submission.to_csv('submissionnew.csv', index=False)\n",
    "\n",
    "# print(\"Submission file created: submissionnew.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "# from sklearn.preprocessing import *\n",
    "# from sklearn.linear_model import HuberRegressor\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "# from sklearn.feature_selection import RFECV\n",
    "\n",
    "# # Feature engineering function\n",
    "# def feature_funk(df):\n",
    "#     sex = {'I': 1, 'F': 3, 'M': 2}\n",
    "#     df['Sex'] = df['Sex'].replace(sex)\n",
    "#     df = df[df['Sex'].isin([1, 2, 3])]\n",
    "#     df.loc[:, 'Sex'] = pd.to_numeric(df['Sex'], errors='coerce')\n",
    "#     return df\n",
    "\n",
    "# # Load and preprocess the training data\n",
    "# train_df = feature_funk(pd.read_csv('train.csv'))\n",
    "\n",
    "# # Define features and target\n",
    "# X = train_df.drop(columns=['id', 'Age'])\n",
    "# y = train_df['Age']\n",
    "\n",
    "# # Preprocessing pipeline\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', Pipeline([\n",
    "#             ('imputer', SimpleImputer(strategy='mean')),\n",
    "#             ('poly', PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)),\n",
    "#             ('scaler', RobustScaler())\n",
    "#         ]), X.columns),\n",
    "#     ])\n",
    "\n",
    "# # Cross-validation setup\n",
    "# cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# # Model with Recursive Feature Elimination and Cross-Validation (RFECV)\n",
    "# selector = RFECV(HuberRegressor(alpha=0.0001, max_iter=100000), step=1, cv=cv, scoring='neg_mean_absolute_error')\n",
    "\n",
    "# pipeline = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('selector', selector),\n",
    "#     ('model', HuberRegressor(alpha=0.0001, max_iter=100000))\n",
    "# ])\n",
    "\n",
    "# # Train-test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# # Cross-validation to assess the model\n",
    "# cv_scores = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring='neg_mean_absolute_error')\n",
    "# mean_cv_mae = -np.mean(cv_scores)\n",
    "\n",
    "# # Fit the model on the training set\n",
    "# pipeline.fit(X_train, y_train)\n",
    "\n",
    "# # Predictions and evaluation on the test set\n",
    "# y_pred = pipeline.predict(X_test)\n",
    "# mae = mean_absolute_error(y_test, y_pred)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# # Output the evaluation metrics\n",
    "# print(f'Cross-validated MAE: {mean_cv_mae:.4f}')\n",
    "# print(f'Test MAE: {mae:.4f}')\n",
    "# print(f'Test MSE: {mse:.4f}')\n",
    "# print(f'Test R^2 Score: {r2:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
